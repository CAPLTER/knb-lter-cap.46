---
title: knb-lter-cap.46
author: CAP LTER
---

# README

content moved to README.md

# isolate SRBP core site surveys

Generate a temporary reference table of surveys at SRBP sites that are part of
the CORE set of surveys (i.e., not SRBP surveys at those same sites).

```{r}
#| eval: TRUE
#| label: isolate_SRBP_core_site_surveys

DBI::dbExecute(
  conn      = pg,
  statement = "
  CREATE TEMPORARY TABLE srbp_core_ids AS
      SELECT
        surveys.id,
        sites.site_code,
        sites.location_type,
        surveys.survey_date
      FROM core_birds.surveys
      JOIN core_birds.sites ON (surveys.site_id = sites.id)
      JOIN (
        SELECT
          subquery.site_id,
          subquery.site_code,
          subquery.reach,
          GREATEST(
            COALESCE(subquery.begin_date, '2000-01-01'),
            COALESCE(subquery.begin, '2000-01-01')
          ) AS begin_date,
          LEAST(
            COALESCE(subquery.end_date, DATE(NOW())),
            COALESCE(subquery.end, DATE(NOW()))
          ) AS end_date
        FROM(
          SELECT
            srcs.site_id,
            sites.site_code,
            srcs.reach,
            srcs.begin_date,
            srcs.end_date,
            CASE
              WHEN srcs.end_date IS NULL THEN DATE('2000-01-01')
            END AS begin,
            CASE
              WHEN srcs.end_date IS NULL THEN DATE(NOW())
            END AS end
          FROM core_birds.salt_river_core_sites srcs
          JOIN core_birds.sites ON (sites.id = srcs.site_id)
        ) AS subquery
        ) AS core_site_date ON (
        surveys.site_id = core_site_date.site_id AND
        surveys.survey_date BETWEEN core_site_date.begin_date AND core_site_date.end_date
      )
      WHERE
      (
        EXTRACT(MONTH FROM surveys.survey_date) IN (1, 2, 3, 4) OR (
        EXTRACT(MONTH FROM surveys.survey_date) IN (5) AND EXTRACT(DAY FROM surveys.survey_date) <= 20
        )
      ) OR
      EXTRACT(MONTH FROM surveys.survey_date) IN (12) AND EXTRACT(DAY FROM surveys.survey_date) >= 15
  ;
  ")

```

# bird observations

## observations: query

```{r}
#| eval: TRUE
#| label: bird_observations_query

bird_observations <- DBI::dbGetQuery(
    conn      = pg,
    statement = "
    SELECT
      surveys.id AS survey_id,
      sites.site_code,
      surveys.survey_date,
      surveys.time_start,
      surveys.time_end,
      observers.observer,
      bird_taxa.code,
      bird_taxa.common_name,
      bird_observations.distance,
      bird_observations.bird_count,
      bird_observations.notes AS observation_notes,
      bird_observations.seen,
      bird_observations.heard,
      bird_observations.direction,
      bird_observations.qccomment
    FROM core_birds.surveys
    JOIN core_birds.sites ON (surveys.site_id = sites.id)
    JOIN core_birds.bird_observations ON (surveys.id = bird_observations.survey_id)
    JOIN core_birds.bird_taxa ON (bird_observations.bird_taxon_id = bird_taxa.id)
    JOIN core_birds.observers ON (observers.id = surveys.observer_id)
    WHERE
      sites.location_type ~~* 'ESCA' OR
      sites.location_type ~~* 'riparian' OR
      sites.location_type ~~* 'NDV' OR
      sites.location_type ~~* 'desert_fertilization' OR
      sites.location_type ~~* 'PASS' OR
      surveys.id IN (SELECT id FROM srbp_core_ids)
    ORDER BY survey_date
    LIMIT 500000
    ;
    "
)

# convert any missing to NA
bird_observations[bird_observations == ""] <- NA

```

Convert observer names to first two letters of each name, including a middle
name if provided. This approach provides anonymity yet distinctly identifies
each birder if needed as a covariate. Pulling this code out separately owing to
its verbosity for a singular purpose: getting the first two letters of the
first and last names of each observers, and presenting those instead of the
full name.

## observations: observers

```{r}
#| eval: TRUE
#| label: bird_observations_observers

bird_observations <- bird_observations |>
  tidyr::separate(observer, c("name1", "name2", "name3"), " ", remove = T) |>
  dplyr::mutate(
    namePart1 = tools::toTitleCase(stringr::str_extract(name1, "\\b\\w{2}")),
    namePart2 = tools::toTitleCase(stringr::str_extract(name2, "\\b\\w{2}")),
    namePart3 = tools::toTitleCase(stringr::str_extract(name3, "\\b\\w{2}"))
    ) |>
  dplyr::mutate(
    observer = dplyr::case_when(
      is.na(namePart3) ~ paste0(namePart1, namePart2),
      !is.na(namePart3) ~ paste0(namePart1, namePart2, namePart3)
    )
    ) |>
  dplyr::select(
    -name1,
    -name2,
    -name3,
    -contains("namePart")
  )

```

## observations: conversions

```{r}
#| eval = TRUE
#| label = bird_observations_conversions

bird_observations <- bird_observations |>
  dplyr::mutate(
    survey_id   = as.character(survey_id),
    survey_date = as.Date(survey_date),
    distance    = as.factor(distance),
    seen        = as.factor(seen),
    heard       = as.factor(heard),
    direction   = as.factor(direction)
    ) |>
  dplyr::select(
    survey_id:time_end,
    observer,
    code:last_col()
  )

```

## observations: filter

The `core-birds-SQL` chunk will query all observation data in the database but
these are not necessarily QC'd; rather than editing the SQL query, we can cull
the full sample set to only those data that are QC'd in this workflow.
Addressed at this step as survey_date has been explicitly declared a date type
in the above chunk.

```{r}
#| eval = FALSE
#| label = bird_observations_filter

bird_observations <- bird_observations |>
  dplyr::filter(survey_date <= "2024-02-22")

```

## observations: data table

`bird_count` is the only numeric variable in `bird_observations` with a current
maximum of 3200, which is unlikely to be surpassed so updating
`bird_observations` attributes is generally not needed.

```{r}
#| eval = TRUE
#| label = bird_observations_DT

# capeml::update_attributes(bird_observations)

# try({
#   capeml::write_attributes(bird_observations, overwrite = FALSE)
#   capeml::write_factors(bird_observations, overwrite = FALSE)
# })

```

# bird surveys

## surveys: query

```{r}
#| eval = TRUE
#| echo = TRUE
#| label = bird_surveys_query

bird_surveys <- DBI::dbGetQuery(
  conn      = pg,
  statement = "
  SELECT
    surveys.id AS survey_id,
    sites.site_code,
    sites.location_type,
    surveys.survey_date,
    surveys.time_start,
    surveys.time_end,
    observers.observer,
    surveys.wind_speed,
    surveys.wind_dir,
    surveys.air_temp,
    surveys.cloud_cover,
    surveys.notes AS survey_notes,
    surveys.human_activity_notes,
    surveys.wind,
    surveys.precipitation,
    surveys.disturbances,
    surveys.sight_obstruct,
    surveys.noise_level,
    surveys.site_condition,
    surveys.non_bird_species,
    additional_birds.additional_bird_observations
  FROM core_birds.surveys
  JOIN core_birds.sites ON (surveys.site_id = sites.id)
  JOIN core_birds.observers ON (observers.id = surveys.observer_id)
  LEFT JOIN (
    SELECT
      survey_id,
      STRING_AGG(bird_taxa.code, '; ') AS additional_bird_observations
    FROM core_birds.additional_bird_observations
    JOIN core_birds.bird_taxa ON (bird_taxa.id = additional_bird_observations.bird_taxon_id)
    GROUP BY survey_id
  ) AS additional_birds ON (additional_birds.survey_id = surveys.id)
  WHERE
    sites.location_type ~~* 'ESCA' OR
    sites.location_type ~~* 'riparian' OR
    sites.location_type ~~* 'NDV' OR
    sites.location_type ~~* 'desert_fertilization' OR
    sites.location_type ~~* 'PASS' OR
    surveys.id IN (SELECT id FROM srbp_core_ids)
  ORDER BY survey_date
  LIMIT 500000
  ;
")

# convert any missing to NA
bird_surveys[bird_surveys == ""] <- NA
  
```

## surveys: observers

```{r}
#| eval = TRUE
#| label = bird_surveys_observers

bird_surveys <- bird_surveys |>
  tidyr::separate(observer, c("name1", "name2", "name3"), " ", remove = T) |>
  dplyr::mutate(
    namePart1 = tools::toTitleCase(stringr::str_extract(name1, "\\b\\w{2}")),
    namePart2 = tools::toTitleCase(stringr::str_extract(name2, "\\b\\w{2}")),
    namePart3 = tools::toTitleCase(stringr::str_extract(name3, "\\b\\w{2}"))
    ) |>
  dplyr::mutate(
    observer = dplyr::case_when(
      is.na(namePart3) ~ paste0(namePart1, namePart2),
      !is.na(namePart3) ~ paste0(namePart1, namePart2, namePart3)
    )
    ) |>
  dplyr::select(
    -name1,
    -name2,
    -name3,
    -contains("namePart")
  )

```

## surveys: conversions

```{r}
#| eval = TRUE
#| label = bird_surveys_conversions

bird_surveys <- bird_surveys |>
  dplyr::mutate(
    survey_id     = as.character(survey_id),
    survey_date   = as.Date(survey_date),
    location_type = as.factor(location_type),
    wind_dir      = as.factor(wind_dir),
    wind          = as.factor(wind),
    precipitation = as.factor(precipitation),
    disturbances  = as.factor(disturbances),
    noise_level   = as.factor(noise_level)
    ) |>
  dplyr::select(
    survey_id:time_end,
    observer,
    wind_speed:last_col()
  )

```

## surveys: filter

The `core-birds-SQL` chunk will query all observation data in the database but
these are not necessarily QC'd; rather than editing the SQL query, we can cull
the full sample set to only those data that are QC'd in this workflow.
Addressed at this step as survey_date has been explicitly declared a date type
in the above chunk.

```{r}
#| eval = FALSE
#| label = bird_surveys_filter

bird_surveys <- bird_surveys |>
  dplyr::filter(survey_date <= "2024-02-22")

```

## surveys: data table

Weather conditions are the only numeric variables among `bird_surveys`, with
new values unlikely to exceed the current ranges of values so updating
`bird_surveys` attributes is generally not needed.

```{r}
#| eval = TRUE
#| label = bird_surveys

# capeml::update_attributes(bird_surveys)

# try({
#   capeml::write_attributes(bird_surveys, overwrite = FALSE)
#   capeml::write_factors(bird_surveys, overwrite = FALSE)
# })

```


# bird locations

Get the bird survey locations - here we are extracting these data from the
database as opposed to using an existing shapefile as I am presenting only the
most up-to-date location information (as opposed to the locations and their
changes through time). Note that I am using only year to reflect the most
recent location: if a site moved twice in a year, month would have to be
considered as well. Also note that I had to include blh.end_date_year in the
query to be able to include it in the HAVING clause.

These spatial data reflect the locations updated by Shero in spring 2013.
However, these data lacked M-9, which is an old site (2000-2001) but referenced
in these bird data (note that I have excluded the volunteer sites, which are
all of those really odd, early sites). The previous published spatial data
(knb-lter-cap.160) included M-9 but did not reflect any updates through 2013. I
merged M-9 from the 160 dataset with the updated through 2013 data, and
exported it to KML for inclusion here. These spatial data include also the most
up-to-date SRBP core sites (core sites only!). I will update 160 with a
reference to this data set.

The purpose of this query is to:

1. use the NOT NULL record if any end_date_year in a site group is NOT NULL
2. use the MAX end_date_year record if all records in a site group have an
   end_date_year
3. SRBP sites are in their own query to ease referencing the
   salt_river_core_sites table

## locations: query

```{r}
#| eval = TRUE
#| label = bird_locations_query

bird_survey_locations <- DBI::dbGetQuery(
  conn      = pg,
  statement = "
  (
    SELECT
      -- sites.id,
      sites.site_code,
      location_type,
      lh.lat,
      lh.long,
      lh.begin_date,
      lh.begin_date_month,
      lh.begin_date_year,
      lh.end_date,
      lh.end_date_month,
      lh.end_date_year
    FROM core_birds.location_histories lh
    JOIN core_birds.sites ON (sites.id = lh.site_id)
    WHERE location_type != 'SRBP'
  )
  UNION
  (
    SELECT
      -- sites.id,
      sites.site_code,
      location_type,
      lh.lat,
      lh.long,
      lh.begin_date,
      lh.begin_date_month,
      lh.begin_date_year,
      lh.end_date,
      lh.end_date_month,
      lh.end_date_year
    FROM core_birds.location_histories lh
    JOIN core_birds.sites ON (sites.id = lh.site_id)
    WHERE sites.id IN (
      SELECT
        site_id
      FROM core_birds.salt_river_core_sites
    )
  )
  ORDER BY site_code
  ;
")

```

**Ensure** that the output contains only the essential elements that need to be
included in a kml, and be sure to check the kml - had a problem in an earlier
version where only some of the site codes were being written to the kml. Not
certain why but it could have been due to the fact that count_any_null and
end_date_year were in fact not being excluded, so there were a lot more spatial
entities than site codes.

## locations: conversions

```{r}
#| eval = TRUE
#| label = locations_conversions

bird_survey_locations <- bird_survey_locations |>
  dplyr::mutate(
    location_type    = as.factor(location_type),
    begin_date_month = as.integer(begin_date_month),
    begin_date_year  = as.integer(begin_date_year),
    end_date_month   = as.integer(end_date_month),
    end_date_year    = as.integer(end_date_year)
  )

```

## locations: data table

Site latitudes and longitudes are the only numeric variables among
`bird_survey_locations`. The current ranges of those variables would only
change if a new site was added. As such, updating `bird_survey_locations`
attributes is generally not needed.

```{r}
#| eval = TRUE
#| label = bird_survey_locations

# capeml::update_attributes(bird_survey_locations)

# try({
#   capeml::write_attributes(bird_survey_locations, overwrite = FALSE)
#   capeml::write_factors(bird_survey_locations, overwrite = FALSE)
# })

```

# locations: as spatial

Locations presented only as tabular data beginning with version 19, code is
left here for posterity and possible future use.

## locations (spatial): SF

```{r}
#| eval = TRUE
#| label = bird_survey_locations_convert_to_spatial

bird_survey_locations_spatial <- bird_survey_locations |>
  dplyr::mutate(location_type = as.character(location_type))

bird_survey_locations_spatial <- sf::st_as_sf(
  x      = bird_survey_locations_spatial,
  coords = c("long", "lat"),
  crs    = 4326
)

```

## locations (spatial): vector

```{r}
#| eval = TRUE
#| label = bird_survey_locations_create_vector

# capeml::update_attributes(bird_survey_locations_spatial)

# try({
#   capeml::write_attributes(bird_survey_locations_spatial, overwrite = FALSE)
# })

bird_survey_locations_spatial_desc <- "bird survey locations at select locations in and around the greater Phoenix metropolitan area"

core_bird_locations_SV <- capemlGIS::create_vector(
  vector_nam  = bird_survey_locations_spatial,
  description = bird_survey_locations_spatial_desc,
  driver      = "GeoJSON"
)

```


# coverages

As the only practical way to present the birding location with their histories
is in tabular form, those data are now presented as a table with their
latitudes and longitudes, a spatialVector is not included. Correspondingly,
only a single bounding box for all sites is included for the geographic
coverage (rather than a point for each site); commented code is included to
detail coverages as point data but required would be a way to distinguish sites
that have multiple locations (i.e., through time, e.g., W-xx) as, as currently
configured, the site_code is the description.

```{r}
#| eval = TRUE
#| label = coverages

begindate <- as.character(min(bird_surveys$survey_date))
enddate   <- as.character(max(bird_surveys$survey_date))
geo_desc  <- yaml::yaml.load_file("config.yaml")$geographic_description

coverage <- EML::set_coverage(
  begin                 = begindate,
  end                   = enddate,
  geographicDescription = geo_desc,
  west                  = sf::st_bbox(bird_survey_locations_spatial)[["xmin"]],
  east                  = sf::st_bbox(bird_survey_locations_spatial)[["xmax"]],
  north                 = sf::st_bbox(bird_survey_locations_spatial)[["ymax"]],
  south                 = sf::st_bbox(bird_survey_locations_spatial)[["ymin"]]
)

```

## taxonomic coverage

*Note* that the `taxa_map.csv` built with the `create_taxa_map()` function and
resolving taxonomic IDs (i.e., `resolve_comm_taxa()`) only needs to be run once
per version/session -- the taxonomicCoverage can be built as many times as
needed with `resolve_comm_taxa()` once the `taxa_map.csv` has been generated
and the taxonomic IDs resolved.

```{r}
#| eval = TRUE
#| label = set_taxonomic_coverage

my_path <- getwd() # taxonomyCleanr requires a path (to build the taxa_map)

# create or update map. A taxa_map.csv is the heart of taxonomyCleanr. This
# function will build the taxa_map.csv and put it in the path identified with
# my_path.

taxonomyCleanr::create_taxa_map(
  path = my_path,
  x    = bird_observations |>
  dplyr::mutate(
    common_name = gsub("unidentified", "", common_name, ignore.case = TRUE),
    common_name = stringr::str_trim(common_name, side = c("both"))
    ),
  col  = "common_name"
)

# resolve_comm_taxa will resolve the taxa by attempting to match the taxon's
# common name. The parameter data.source 3 is ITIS, which is the only authority
# taxonomyCleanr will allow for common names.

taxonomyCleanr::resolve_comm_taxa(
  path         = my_path,
  data.sources = 3 # ITIS
)

# build the EML taxonomomic coverage
taxaCoverage <- taxonomyCleanr::make_taxonomicCoverage(path = my_path)

# add taxonomic to other coverages
coverage$taxonomicCoverage <- taxaCoverage

```


# dataset

```{r}
#| eval = TRUE
#| label = construct_dataset

dataset <- capeml::create_dataset()
```

# eml

```{r}
#| eval = TRUE
#| label = construct_eml

eml <- capeml::create_eml()
```

```{r}
#| eval = TRUE
#| label = validate_eml

EML::eml_validate(eml)
```

```{r}
#| eval = TRUE
#| label = eml_to_file

capeml::write_cap_eml()
```


# file placement

```{r}
#| eval = TRUE
#| label = preview_data_file_to_upload

(data_files_to_upload <- list.files(pattern = "^46_"))
```

Move data and final xml files to respective ASU locations.

```{r}
#| eval = TRUE
#| label = S3_helper_functions

source("~/Documents/localSettings/aws.s3")
```

```{r}
#| eval = TRUE
#| label = upload_data_S3

lapply(data_files_to_upload, capeml::data_to_amz)
```

# EDI

## EDI: login

```{r}
#| eval = TRUE
#| label = edi_login

source("~/Documents/localSettings/edi.R")
```

## EDI: evaluate

```{r}
#| eval = TRUE
#| echo = TRUE
#| message = TRUE
#| label = edi_evaluate

capeml::get_package_evaluation(full_report = FALSE)
# report <- capeml::get_package_evaluation(full_report = TRUE)
```

## EDI: update

```{r}
#| eval = TRUE
#| label = edi_update

capeml::create_package(
  environment = "production",
  update      = TRUE
)
```

## EDI: logout

```{r}
#| eval = TRUE
#| label = edi_logout

EDIutils::logout()
```


# post processing

remove data files (if desired)

```{r}
#| eval = TRUE
#| label = delete_data_files

file.remove(data_files_to_upload)
```

XML/EML file to Amazon and cap-metadata

```{r}
#| eval = TRUE
#| label = delete_data_files

capeml::eml_to_amz(list.files(pattern = "knb.+xml"))

file.copy(list.files(pattern = "knb.+xml"), "/home/srearl/localRepos/cap-metadata/cap-data-eml/")
file.remove(list.files(pattern = "knb.+xml"))
```