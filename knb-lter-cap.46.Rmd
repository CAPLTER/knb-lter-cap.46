---
date: Sys.Date()
output: html_document
editor_options: 
  chunk_output_type: console
---

# README

content moved to README.md


# bird observations

## observations: query

```{r bird-observations-query, eval=TRUE}

bird_observations <- DBI::dbGetQuery(pg, "
SELECT
  surveys.id AS survey_id,
  sites.site_code,
  surveys.survey_date,
  surveys.time_start,
  surveys.time_end,
  observers.observer,
  bird_taxa.code,
  bird_taxa.common_name,
  bird_observations.distance,
  bird_observations.bird_count,
  bird_observations.notes AS observation_notes,
  bird_observations.seen,
  bird_observations.heard,
  bird_observations.direction,
  bird_observations.qccomment
FROM core_birds.surveys
JOIN core_birds.sites ON (surveys.site_id = sites.id)
JOIN core_birds.bird_observations ON (surveys.id = bird_observations.survey_id)
JOIN core_birds.bird_taxa ON (bird_observations.bird_taxon_id = bird_taxa.id)
JOIN core_birds.observers ON (observers.id = surveys.observer_id)
WHERE
  sites.location_type ~~* 'ESCA' OR
  sites.location_type ~~* 'riparian' OR
  sites.location_type ~~* 'NDV' OR
  sites.location_type ~~* 'desert_fertilization' OR
  sites.location_type ~~* 'PASS' OR
  surveys.id IN (
    SELECT
      srbp_core_ids.id
    FROM
    (
      SELECT
        surveys.id,
        sites.site_code,
        sites.location_type,
        surveys.survey_date
      FROM core_birds.surveys
      JOIN core_birds.sites ON (surveys.site_id = sites.id)
      JOIN (
        SELECT
          subquery.site_id,
          subquery.site_code,
          subquery.reach,
          GREATEST(
            COALESCE(subquery.begin_date, '2000-01-01'),
            COALESCE(subquery.begin, '2000-01-01')
          ) AS begin_date,
          LEAST(
            COALESCE(subquery.end_date, DATE(NOW())),
            COALESCE(subquery.end, DATE(NOW()))
          ) AS end_date
        FROM(
          SELECT
            srcs.site_id,
            sites.site_code,
            srcs.reach,
            srcs.begin_date,
            srcs.end_date,
            CASE
              WHEN srcs.end_date IS NULL THEN DATE('2000-01-01')
            END AS begin,
            CASE
              WHEN srcs.end_date IS NULL THEN DATE(NOW())
            END AS end
          FROM core_birds.salt_river_core_sites srcs
          JOIN core_birds.sites ON (sites.id = srcs.site_id)
        ) AS subquery
        ) AS core_site_date ON (
        surveys.site_id = core_site_date.site_id AND
        surveys.survey_date BETWEEN core_site_date.begin_date AND core_site_date.end_date
      )
      WHERE
      (
        EXTRACT(MONTH FROM surveys.survey_date) IN (1, 2, 3, 4) OR (
        EXTRACT(MONTH FROM surveys.survey_date) IN (5) AND EXTRACT(DAY FROM surveys.survey_date) <= 20
        )
      ) OR
      EXTRACT(MONTH FROM surveys.survey_date) IN (12) AND EXTRACT(DAY FROM surveys.survey_date) >= 15
    ) AS srbp_core_ids
  )
ORDER BY survey_date
LIMIT 500000
;
"
)

# convert any missing to NA
bird_observations[bird_observations == ""] <- NA

```

Convert observer names to first two letters of each name, including a middle
name if provided. This approach provides anonymity yet distinctly identifies
each birder if needed as a covariate. Pulling this code out separately owing to
its verbosity for a singular purpose: getting the first two letters of the
first and last names of each observers, and presenting those instead of the
full name.

## observations: observers

```{r bird-observations-observers, eval=TRUE}

bird_observations <- bird_observations |>
  tidyr::separate(observer, c("name1", "name2", "name3"), " ", remove = T) |>
  dplyr::mutate(
    namePart1 = tools::toTitleCase(stringr::str_extract(name1, "\\b\\w{2}")),
    namePart2 = tools::toTitleCase(stringr::str_extract(name2, "\\b\\w{2}")),
    namePart3 = tools::toTitleCase(stringr::str_extract(name3, "\\b\\w{2}"))
    ) |>
  dplyr::mutate(
    observer = dplyr::case_when(
      is.na(namePart3) ~ paste0(namePart1, namePart2),
      !is.na(namePart3) ~ paste0(namePart1, namePart2, namePart3)
    )
    ) |>
  dplyr::select(
    -name1,
    -name2,
    -name3,
    -contains("namePart")
  )

```

## observations: conversions

```{r bird-observations-conversions, eval=TRUE}

bird_observations <- bird_observations |>
  dplyr::mutate(
    survey_id   = as.character(survey_id),
    survey_date = as.Date(survey_date),
    distance    = as.factor(distance),
    seen        = as.factor(seen),
    heard       = as.factor(heard),
    direction   = as.factor(direction)
    ) |>
  dplyr::select(
    survey_id:time_end,
    observer,
    code:last_col()
  )

```

## observations: filter

The `core-birds-SQL` chunk will query all observation data in the database but
these are not necessarily QC'd; rather than editing the SQL query, we can cull
the full sample set to only those data that are QC'd in this workflow.
Addressed at this step as survey_date has been explicitly declared a date type
in the above chunk.

```{r bird-observations-filter, eval=TRUE}

bird_observations <- bird_observations |>
  dplyr::filter(survey_date <= "2019-09-01")

```

## observations: data table

```{r bird-observations-DT, eval=TRUE}

try({
  capeml::write_attributes(bird_observations, overwrite = FALSE)
  capeml::write_factors(bird_observations, overwrite = FALSE)
})

bird_observations_desc <- "bird survey sampling details (site, date, time, observer) and birds observed during the survey windows (type, number, distance from observer, behavior)"

bird_observations_DT <- capeml::create_dataTable(
  dfname         = bird_observations,
  description    = bird_observations_desc,
  dateRangeField = "survey_date"
)

```

# bird surveys

## surveys: query

```{r bird-surveys-query, eval=TRUE, echo=TRUE, message=TRUE}

bird_surveys <- DBI::dbGetQuery(pg, "
SELECT
  surveys.id AS survey_id,
  sites.site_code,
  sites.location_type,
  surveys.survey_date,
  surveys.time_start,
  surveys.time_end,
  observers.observer,
  surveys.wind_speed,
  surveys.wind_dir,
  surveys.air_temp,
  surveys.cloud_cover,
  surveys.notes AS survey_notes,
  surveys.human_activity_notes,
  surveys.wind,
  surveys.precipitation,
  surveys.disturbances,
  surveys.sight_obstruct,
  surveys.noise_level,
  surveys.site_condition,
  surveys.non_bird_species,
  additional_birds.additional_bird_observations
FROM core_birds.surveys
JOIN core_birds.sites ON (surveys.site_id = sites.id)
JOIN core_birds.observers ON (observers.id = surveys.observer_id)
LEFT JOIN (
  SELECT
    survey_id,
    STRING_AGG(bird_taxa.code, '; ') AS additional_bird_observations
  FROM core_birds.additional_bird_observations
  JOIN core_birds.bird_taxa ON (bird_taxa.id = additional_bird_observations.bird_taxon_id)
  GROUP BY survey_id
) AS additional_birds ON (additional_birds.survey_id = surveys.id)
WHERE
  sites.location_type ~~* 'ESCA' OR
  sites.location_type ~~* 'riparian' OR
  sites.location_type ~~* 'NDV' OR
  sites.location_type ~~* 'desert_fertilization' OR
  sites.location_type ~~* 'PASS' OR
  surveys.id IN (
    SELECT
      srbp_core_ids.id
    FROM
    (
      SELECT
        surveys.id,
        sites.site_code,
        sites.location_type,
        surveys.survey_date
      FROM core_birds.surveys
      JOIN core_birds.sites ON (surveys.site_id = sites.id)
      JOIN (
        SELECT
          subquery.site_id,
          subquery.site_code,
          subquery.reach,
          GREATEST(
            COALESCE(subquery.begin_date, '2000-01-01'),
            COALESCE(subquery.begin, '2000-01-01')
          ) AS begin_date,
          LEAST(
            COALESCE(subquery.end_date, DATE(NOW())),
            COALESCE(subquery.end, DATE(NOW()))
          ) AS end_date
        FROM(
          SELECT
            srcs.site_id,
            sites.site_code,
            srcs.reach,
            srcs.begin_date,
            srcs.end_date,
            CASE
              WHEN srcs.end_date IS NULL THEN DATE('2000-01-01')
            END AS begin,
            CASE
              WHEN srcs.end_date IS NULL THEN DATE(NOW())
            END AS end
          FROM core_birds.salt_river_core_sites srcs
          JOIN core_birds.sites ON (sites.id = srcs.site_id)
        ) AS subquery
        ) AS core_site_date ON (
        surveys.site_id = core_site_date.site_id AND
        surveys.survey_date BETWEEN core_site_date.begin_date AND core_site_date.end_date
      )
      WHERE
      (
        EXTRACT(MONTH FROM surveys.survey_date) IN (1, 2, 3, 4) OR (
        EXTRACT(MONTH FROM surveys.survey_date) IN (5) AND EXTRACT(DAY FROM surveys.survey_date) <= 20
        )
      ) OR
      EXTRACT(MONTH FROM surveys.survey_date) IN (12) AND EXTRACT(DAY FROM surveys.survey_date) >= 15
    ) AS srbp_core_ids
  )
ORDER BY survey_date
LIMIT 500000
;
")

# convert any missing to NA
bird_surveys[bird_surveys == ""] <- NA
  
```

## surveys: observers

```{r bird-surveys-observers, eval=TRUE}

bird_surveys <- bird_surveys |>
  tidyr::separate(observer, c("name1", "name2", "name3"), " ", remove = T) |>
  dplyr::mutate(
    namePart1 = tools::toTitleCase(stringr::str_extract(name1, "\\b\\w{2}")),
    namePart2 = tools::toTitleCase(stringr::str_extract(name2, "\\b\\w{2}")),
    namePart3 = tools::toTitleCase(stringr::str_extract(name3, "\\b\\w{2}"))
    ) |>
  dplyr::mutate(
    observer = dplyr::case_when(
      is.na(namePart3) ~ paste0(namePart1, namePart2),
      !is.na(namePart3) ~ paste0(namePart1, namePart2, namePart3)
    )
    ) |>
  dplyr::select(
    -name1,
    -name2,
    -name3,
    -contains("namePart")
  )

```

## surveys: conversions

```{r bird-surveys-conversions, eval=TRUE}

bird_surveys <- bird_surveys |>
  dplyr::mutate(
    survey_id     = as.character(survey_id),
    survey_date   = as.Date(survey_date),
    location_type = as.factor(location_type),
    wind_dir      = as.factor(wind_dir),
    wind          = as.factor(wind),
    precipitation = as.factor(precipitation),
    disturbances  = as.factor(disturbances),
    noise_level   = as.factor(noise_level)
    ) |>
  dplyr::select(
    survey_id:time_end,
    observer,
    wind_speed:last_col()
  )

```

## surveys: filter

The `core-birds-SQL` chunk will query all observation data in the database but
these are not necessarily QC'd; rather than editing the SQL query, we can cull
the full sample set to only those data that are QC'd in this workflow.
Addressed at this step as survey_date has been explicitly declared a date type
in the above chunk.

```{r bird-surveys-filter, eval=TRUE}

bird_surveys <- bird_surveys |>
  dplyr::filter(survey_date <= "2019-09-01")

```

## surveys: data table

```{r bird-surveys, eval=TRUE}

dplyr::glimpse(bird_surveys)

try({
  capeml::write_attributes(bird_surveys, overwrite = FALSE)
  capeml::write_factors(bird_surveys, overwrite = FALSE)
})

bird_surveys_desc <- "bird survey sampling details including survey location, date, times, observer, conditions at the site during the survey, and birds and other animals observed at the survey site but outside the designated survey window"

bird_surveys_DT <- capeml::create_dataTable(
  dfname         = bird_surveys,
  description    = bird_surveys_desc,
  dateRangeField = "survey_date"
)

```


# bird locations

Get the bird survey locations - here we are extracting these data from the
database as opposed to using an existing shapefile as I am presenting only the
most up-to-date location information (as opposed to the locations and their
changes through time). Note that I am using only year to reflect the most
recent location: if a site moved twice in a year, month would have to be
considered as well. Also note that I had to include blh.end_date_year in the
query to be able to include it in the HAVING clause.

These spatial data reflect the locations updated by Shero in spring 2013.
However, these data lacked M-9, which is an old site (2000-2001) but referenced
in these bird data (note that I have excluded the volunteer sites, which are
all of those really odd, early sites). The previous published spatial data
(knb-lter-cap.160) included M-9 but did not reflect any updates through 2013. I
merged M-9 from the 160 dataset with the updated through 2013 data, and
exported it to KML for inclusion here. These spatial data include also the most
up-to-date SRBP core sites (core sites only!). I will update 160 with a
reference to this data set.

The purpose of this query is to:

1. use the NOT NULL record if any end_date_year in a site group is NOT NULL
2. use the MAX end_date_year record if all records in a site group have an
   end_date_year
3. SRBP sites are in their own query to ease referencing the
   salt_river_core_sites table

## locations: query

```{r bird-locations-query, eval=TRUE}

bird_survey_locations <- DBI::dbGetQuery(pg, "
(
  SELECT
    -- sites.id,
    sites.site_code,
    location_type,
    lh.lat,
    lh.long,
    lh.begin_date,
    lh.begin_date_month,
    lh.begin_date_year,
    lh.end_date,
    lh.end_date_month,
    lh.end_date_year
  FROM core_birds.location_histories lh
  JOIN core_birds.sites ON (sites.id = lh.site_id)
  WHERE location_type != 'SRBP'
)
UNION
(
  SELECT
    -- sites.id,
    sites.site_code,
    location_type,
    lh.lat,
    lh.long,
    lh.begin_date,
    lh.begin_date_month,
    lh.begin_date_year,
    lh.end_date,
    lh.end_date_month,
    lh.end_date_year
  FROM core_birds.location_histories lh
  JOIN core_birds.sites ON (sites.id = lh.site_id)
  WHERE sites.id IN (
    SELECT
      site_id
    FROM core_birds.salt_river_core_sites
  )
)
ORDER BY site_code
;
")

```

**Ensure** that the output contains only the essential elements that need to be
included in a kml, and be sure to check the kml - had a problem in an earlier
version where only some of the site codes were being written to the kml. Not
certain why but it could have been due to the fact that count_any_null and
end_date_year were in fact not being excluded, so there were a lot more spatial
entities than site codes.

## locations: conversions

```{r locations-conversions, eval=TRUE}

bird_survey_locations <- bird_survey_locations |>
  dplyr::mutate(
    location_type    = as.factor(location_type),
    begin_date_month = as.integer(begin_date_month),
    begin_date_year  = as.integer(begin_date_year),
    end_date_month   = as.integer(end_date_month),
    end_date_year    = as.integer(end_date_year)
  )

```

## locations: data table

```{r bird-survey-locations, eval=TRUE}

dplyr::glimpse(bird_survey_locations)

try({
  capeml::write_attributes(bird_survey_locations, overwrite = FALSE)
  capeml::write_factors(bird_survey_locations, overwrite = FALSE)
})

bird_survey_locations_desc <- "Geographic position of bird survey locations. The precise position of many survey locations has changed over the course of the study, either intentionally to accommodate changes at or near the study site or indirectly reflecting updated GPS positions. Survey sites that have moved have an associated end date corresponding to when the position moved or was recalibrated, either as a date or month and year if the exact day of repositioning is not known. Begin dates correspond to the earliest survey at that position or, in the case of sites that have moved (i.e., have an end date), the earliest survey after the end date."

bird_survey_locations_DT <- capeml::create_dataTable(
  dfname      = bird_survey_locations,
  description = bird_survey_locations_desc
)

```

## locations: as spatial

Locations presented only as tabular data beginning with version 19, code is
left here for posterity and possible future use.

```{r bird_survey_locations-convert-to-spatial, eval=FALSE}

bird_survey_locations <- sf::st_as_sf(
  x      = bird_survey_locations,
  coords = c("long", "lat"),
  crs    = 4326
)

try({
  capeml::write_attributes(bird_survey_locations, overwrite = FALSE)
})

core_bird_locations_desc <- "bird survey locations at select locations in and around the greater Phoenix metropolitan area"

core_bird_locations_SV <- capemlGIS::create_vector(
  vector_nam     = bird_survey_locations,
  description    = core_bird_locations_desc,
  driver         = "GeoJSON"
)

```

# people

```{r people}

# S. Lerman constructed separately to accommodate middle initial

susannahOrcid        <- EML::eml$userId(directory = "https://orcid.org")
susannahOrcid$userId <- "0000-0002-2331-8439"

susannah <- EML::eml$creator(
  individualName = EML::eml$individualName(
    givenName = c("Susannah", "B"),
    surName   = "Lerman"),
  electronicMailAddress = "slerman@cns.umass.edu",
  organizationName      = "USDA Forest Service Northern Research Station",
  userId                = susannahOrcid
)

paige <- gioseml::create_role(
  firstName = "paige",
  lastName  = "warren",
  roleType  = "creator"
)

heather <- gioseml::create_role(
  firstName = "hea",
  lastName  = "bateman",
  roleType  = "creator"
)

madhu <- gioseml::create_role(
  firstName = "ma",
  lastName  = "katti",
  roleType  = "creator"
)

eyal <- gioseml::create_role(
  firstName = "eyal",
  lastName  = "shochat",
  roleType  = "creator"
)

creators <- list(
  paige,
  susannah,
  heather,
  madhu,
  eyal
)

stevan <- gioseml::create_role(
  firstName = "stevan",
  lastName  = "earl",
  roleType  = "metadata"
)
metadataProvider <- list(stevan)

```

# coverages

As the only practical way to present the birding location with their histories
is in tabular form, those data are now presented as a table with their
latitutes and longitudes, a spatialVector is not included. Correspondingly,
only a single bounding box for all sites is included for the geographic
coverage (rather than a point for each site); commented code is included to
detail coverages as point data but required would be a way to distinguish sites
that have multiple locations (i.e., through time, e.g., W-xx) as, as currently
configured, the site_code is the description.

```{r coverages}

begindate <- as.character(min(bird_surveys$survey_date))
enddate   <- as.character(max(bird_surveys$survey_date))
geo_desc  <- yaml::yaml.load_file("config.yaml")$geographicCoverage$geographicDescription

coverage <- EML::set_coverage(
  begin                 = begindate,
  end                   = enddate,
  geographicDescription = geo_desc,
  west                  = min(bird_survey_locations$long),
  east                  = max(bird_survey_locations$long),
  north                 = max(bird_survey_locations$lat),
  south                 = min(bird_survey_locations$lat)
)

# as_spatial <- sf::st_as_sf(
#   x = bird_survey_locations |>
#   dplyr::group_by(site_code) |>
#   dplyr::summarise(
#     lat = max(lat),
#     long = max(long)
#     ),
#   coords = c("long", "lat"),
#   crs    = 4326
# )
# 
# geographic_coverage <- split(
#   x = as_spatial,
#   f = as_spatial$site_code
# ) |>
# {\(site) purrr::map(.x = site, ~ capeml::create_geographic_coverage(.x, description = .x$site_code))}() |>
# unname()

# coverage <- list(
#   temporalCoverage = list(
#     rangeOfDates = list(
#       beginDate = list(
#         calendarDate = begin_date
#         ),
#       endDate = list(
#         calendarDate = end_date
#       )
#     )
#   ), # close temporalCoverage
# geographicCoverage = geographic_coverage
# ) # close coverage

```

## taxonomic coverage

*Note* that the `taxa_map.csv` built with the `create_taxa_map()` function and
resolving taxonomic IDs (i.e., `resolve_comm_taxa()`) only needs to be run once
per version/session -- the taxonomicCoverage can be built as many times as
needed with `resolve_comm_taxa()` once the `taxa_map.csv` has been generated
and the taxonomic IDs resolved.

```{r set_taxonomic_coverage, eval=TRUE}

my_path <- getwd() # taxonomyCleanr requires a path (to build the taxa_map)

# create or update map. A taxa_map.csv is the heart of taxonomyCleanr. This
# function will build the taxa_map.csv and put it in the path identified with
# my_path.
taxonomyCleanr::create_taxa_map(
  path = my_path,
  x    = bird_observations,
  col  = "common_name"
)

# resolve_comm_taxa will resolve the taxa by attempting to match the taxon's
# common name - note that this function applies only to common names. There is
# a separate function for matching scientific names. In this case, data.source
# 3 is ITIS, which is the only authority taxonomyCleanr will allow for common
# names.
taxonomyCleanr::resolve_comm_taxa(path = my_path, data.sources = 3) # in this case, 3 is ITIS

# build the EML taxonomomic coverage
taxaCoverage <- taxonomyCleanr::make_taxonomicCoverage(path = my_path)

# add taxonomic to other coverages
coverage$taxonomicCoverage <- taxaCoverage

```

# dataset

```{r construct-dataset}

dataset <- capeml::create_dataset()
```


# eml

```{r construct-eml, eval=TRUE}

eml <- capeml::create_eml()
```

```{r validate-eml, eval=TRUE}

EML::eml_validate(eml)
```

```{r eml-to-file, eval=TRUE}

capeml::write_cap_eml()
```

# file placement

```{r package-details, eval=TRUE}

# retrieve package details from config.yaml
if (!file.exists("config.yaml")) {
  stop("config.yaml not found")
}
packageIdent <- yaml::yaml.load_file("config.yaml")$packageIdent
packageNum   <- yaml::yaml.load_file("config.yaml")$packageNum
```

```{r preview_data_file_to_upload}

# preview data set files that will be uploaded to S3
list.files(pattern = paste0(packageNum, "_"))
```

Move data and final xml files to respective ASU locations.

```{r S3_helper_functions}

# functions and setting for uploading to S3
library(aws.s3)
source("~/Documents/localSettings/aws.s3")
```

```{r upload_data_S3}

# upload files to S3
lapply(list.files(pattern = paste0(packageNum, "_")), gioseml::data_to_amz)
```

```{r clean_up}

# remove data files
dataFilesToRemove <- dir(pattern = paste0(packageNum, "_"))
file.remove(dataFilesToRemove)

# EML to S3
gioseml::eml_to_amz(paste0(packageIdent, ".xml"))

# EML to cap-data-eml and remove file from project
file.copy(
  from = paste0(packageIdent, ".xml"),
  to   = "/home/srearl/localRepos/cap-metadata/cap-data-eml/"
)
file.remove(paste0(packageIdent, ".xml"))

```

# EDI

## EDI: login

```{r edi-login, eval=TRUE, echo=TRUE, message=TRUE}

EDIutils::login(
  userId   = keyring::key_get("edi_user"),
  userPass = keyring::key_get("edi_pass")
)

```

## EDI: evaluate

```{r edi-evaluate, eval=TRUE, echo=TRUE, message=TRUE}

evaluation <- EDIutils::evaluate_data_package(
  eml         = paste0(packageIdent, ".xml"),
  useChecksum = FALSE,
  env         = "production"
)

Sys.sleep(10)

eval_status <- EDIutils::check_status_evaluate(
  transaction = evaluation,
  env         = "production"
)

if (eval_status) {

  # evalution summary

  EDIutils::read_evaluate_report_summary(
    transaction = evaluation,
    env         = "production"
  )

}

# evalution detailed

# EDIutils::read_evaluate_report(
#   transaction = evaluation,
#   env         = "production"
# )

```

## EDI: update

```{r edi-update, eval=TRUE, echo=TRUE, message=TRUE}

EDIutils::update_data_package(
  eml         = paste0(packageIdent, ".xml"),
  env         = "production"
)

```

## EDI: logout

```{r edi-logout, eval=TRUE, echo=TRUE, message=TRUE}

EDIutils::logout()

```
